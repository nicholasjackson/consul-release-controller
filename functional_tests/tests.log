2022-04-05T15:31:05.716+0100 [ERROR] 2022-04-05T15:31:05.716+0100 [DEBUG] Generating TLS Certificates for Ingress: path=/home/nicj/.shipyard/certs
2022-04-05T15:31:11.261+0100 [ERROR] 2022-04-05T15:31:11.261+0100 [DEBUG] Starting Ingress
2022-04-05T15:31:11.261+0100 [ERROR] Running configuration from:  ./shipyard/kubernetes

2022-04-05T15:31:11.261+0100 [DEBUG] Statefile does not exist
2022-04-05T15:31:14.644+0100 [ERROR] 2022-04-05T15:31:14.644+0100 [INFO]  Creating resources from configuration: path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes
2022-04-05T15:31:14.644+0100 [DEBUG] Statefile does not exist
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=CONSUL_CACERT
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=UPSTREAMS
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=TLS_KEY
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=TLS_CERT
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_ADDR
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=GRAFANA_PASSWORD
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Generating template: ref=consul_namespace output=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-05T15:31:17.794+0100 [DEBUG] Template content: ref=consul_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=TEMPO_HTTP_ADDR
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=GRAFANA_USER
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=GRAFANA_HTTP_ADDR
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Generating template: ref=consul_values output=/home/nicj/.shipyard/data/consul_kubernetes/consul_values.yaml
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [DEBUG] Template content: ref=consul_values
  source=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: #{{ .Vars.datacenter }}
  | 
  |   acls:
  |     manageSystemACLs: #{{ .Vars.acl_enabled }}
  |   tls:
  |     enabled: #{{ .Vars.tls_enabled }}
  |     enableAutoEncrypt: #{{ .Vars.tls_enabled }}
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: #{{ .Vars.federation_enabled }}
  |     createFederationSecret: #{{ .Vars.create_federation_secret }}
  | 
  |   image: #{{ .Vars.consul_image }}
  |   
  |   imageK8S: #{{ .Vars.consul_k8s_image }}
  |   
  |   imageEnvoy: #{{ .Vars.consul_envoy_image }}
  | 
  |   metrics:
  |     enabled: #{{ .Vars.metrics_enabled }}
  |     enableAgentMetrics: #{{ .Vars.metrics_enabled }}
  |     enableGatewayMetrics: #{{ .Vars.metrics_enabled }}
  |   
  |   logLevel: #{{ if eq .Vars.debug true }}"debug"#{{ else }}"info"#{{ end }}
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.#{{ .Vars.monitoring_namespace }}.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: #{{ if eq .Vars.debug true }}"--log-level debug"#{{ else }}null#{{ end }}
  | 
  |   transparentProxy:
  |     defaultEnabled: #{{ .Vars.transparent_proxy_enabled }}
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: #{{ .Vars.ingress_gateway_enabled }}
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       #{{ range .Vars.ingress_gateway_ports }}
  |         - port: #{{ . }}
  |           nodePort: null
  |       #{{ end }}
  | 
  | 
  | meshGateway:
  |   enabled: #{{ .Vars.mesh_gateway_enabled }}
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: #{{ .Vars.mesh_gateway_address }}
  |     port: 30443
  | 
  |   service:
  |     enabled: #{{ .Vars.mesh_gateway_enabled }}
  |     type: NodePort
  |     nodePort: 30443
  
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=CONSUL_CAKEY
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Network: ref=dc1
2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=CONSUL_HTTP_TOKEN_FILE
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [DEBUG] Template output: ref=consul_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: consul
  |     labels:
  |       name: consul
  
2022-04-05T15:31:17.794+0100 [INFO]  Generating template: ref=consul_proxy_defaults output=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-05T15:31:17.794+0100 [DEBUG] Template content: ref=consul_proxy_defaults
  source=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.#{{ .Vars.monitoring_namespace}}.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=KUBECONFIG
2022-04-05T15:31:17.794+0100 [INFO]  Generating template: ref=certs_script output=/home/nicj/.shipyard/data/kube_setup/fetch_certs.sh
2022-04-05T15:31:17.794+0100 [DEBUG] Template content: ref=certs_script
  source=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [DEBUG] Template output: ref=consul_proxy_defaults
  destination=
  | ---
  | apiVersion: consul.hashicorp.com/v1alpha1
  | kind: ProxyDefaults
  | metadata:
  |   name: global
  | spec:
  |   config:
  |     envoy_prometheus_bind_addr: '0.0.0.0:9102'
  |     envoy_extra_static_clusters_json: >
  |       {
  |         "name": "tempo",
  |         "type": "STRICT_DNS",
  |         "connect_timeout": "3.000s",
  |         "lb_policy": "ROUND_ROBIN",
  |         "load_assignment": {
  |           "cluster_name": "tempo",
  |           "endpoints": [
  |             {
  |               "lb_endpoints": [
  |                 {
  |                   "endpoint": {
  |                     "address": {
  |                       "socket_address": {
  |                         "address": "tempo.monitoring.svc",
  |                         "port_value": 9411
  |                       }
  |                     }
  |                   }
  |                 }
  |               ]
  |             }
  |           ]
  |         }
  |       }
  |     envoy_tracing_json: >
  |       {
  |         "http": {
  |           "name": "envoy.tracers.zipkin",
  |           "typedConfig": {
  |             "@type": "type.googleapis.com/envoy.config.trace.v3.ZipkinConfig",
  |             "collector_cluster": "tempo",
  |             "collector_endpoint_version": "HTTP_JSON",
  |             "collector_endpoint": "/api/v1/spans",
  |             "shared_span_context": false
  |           }
  |         }
  |       }
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [DEBUG] Template output: ref=certs_script
  destination=
  | #! /bin/sh -e
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.crt"' | \
  | 	base64 -d > /output/tls.crt
  | 
  | kubectl get secret consul-release-controller-certificate -n consul -o json | \
  | 	jq -r '.data."tls.key"' | \
  | 	base64 -d > /output/tls.key
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Generating template: ref=controller_values output=/home/nicj/.shipyard/data/kube_setup/helm-values.yaml
2022-04-05T15:31:17.794+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [DEBUG] Template content: ref=controller_values
  source=
  | controller:
  |   enabled: "#{{ .Vars.controller_enabled }}"
  |   container_config:
  |     image:
  |       repository: "#{{ .Vars.controller_repo }}"
  |       tag: "#{{ .Vars.controller_version }}"
  | autoencrypt:
  |   enabled: #{{ .Vars.tls_enabled }}
  | acls:
  |   enabled: #{{ .Vars.acls_enabled }}
  | #{{- if eq .Vars.controller_enabled false }}
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  | #{{ end }}
2022-04-05T15:31:17.795+0100 [ERROR] 2022-04-05T15:31:17.794+0100 [INFO]  Creating Output: ref=PROMETHEUS_HTTP_ADDR
2022-04-05T15:31:17.795+0100 [DEBUG] Template output: ref=consul_values
  destination=
  | # Available parameters and their default values for the Consul chart.
  | # Server, when enabled, configures a server cluster to run. This should
  | # be disabled if you plan on connecting to a Consul cluster external to
  | # the Kube cluster.
  | global:
  |   # image: hashicorpdev/consul
  |   # imageK8S: hashicorpdev/consul-k8s:crd-controller-base-latest
  |   name: consul
  | 
  |   datacenter: dc1
  | 
  |   acls:
  |     manageSystemACLs: true
  |   tls:
  |     enabled: true
  |     enableAutoEncrypt: true
  |     httpsOnly: false
  | 
  |   federation:
  |     enabled: false
  |     createFederationSecret: false
  | 
  |   image: hashicorp/consul:1.11.3
  |   
  |   imageK8S: hashicorp/consul-k8s-control-plane:0.40.0
  |   
  |   imageEnvoy: envoyproxy/envoy:v1.20.1
  | 
  |   metrics:
  |     enabled: true
  |     enableAgentMetrics: true
  |     enableGatewayMetrics: true
  |   
  |   logLevel: "info"
  | 
  | server:
  |   replicas: 1
  |   bootstrapExpect: 1
  | 
  |   storage: 128Mi
  | 
  |   extraConfig: |
  |     {
  |       "ui_config": {
  |         "enabled": true,
  |         "metrics_provider": "prometheus",
  |         "metrics_proxy": {
  |           "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |         }
  |       }
  |     }
  | 
  | controller:
  |   enabled: true
  | ui:
  |   enabled: true
  | connectInject:
  |   enabled: true
  |   default: false  # true will inject by default, otherwise requires annotation
  |   failurePolicy: "Ignore"
  |   replicas: 1
  |   envoyExtraArgs: null
  | 
  |   transparentProxy:
  |     defaultEnabled: false
  | 
  |   # Requires Consul v1.5+ and consul-k8s v0.8.1+
  |   centralConfig:
  |     enabled: true
  | 
  | ingressGateways:
  |   enabled: true
  |   defaults:
  |     replicas: 1
  |     service:
  |       ports:
  |       
  |         - port: 18080
  |           nodePort: null
  |       
  |         - port: 18443
  |           nodePort: null
  |       
  | 
  | 
  | meshGateway:
  |   enabled: false
  |   replicas: 1
  | 
  |   wanAddress:
  |     source: Static
  |     static: dc1.k8s-cluster.shipyard.run
  |     port: 30443
  | 
  |   service:
  |     enabled: false
  |     type: NodePort
  |     nodePort: 30443
  
2022-04-05T15:31:17.795+0100 [DEBUG] Template output: ref=controller_values
  destination=
  | controller:
  |   enabled: "false"
  |   container_config:
  |     image:
  |       repository: "nicholasjackson/consul-release-controller"
  |       tag: ""
  | autoencrypt:
  |   enabled: true
  | acls:
  |   enabled: true
  | webhook:
  |   service: controller-webhook
  |   namespace: shipyard
  |
2022-04-05T15:31:17.796+0100 [ERROR] 2022-04-05T15:31:17.796+0100 [DEBUG] Attempting to create using bridge plugin: ref=dc1
2022-04-05T15:31:17.819+0100 [ERROR] 2022-04-05T15:31:17.819+0100 [INFO]  Creating ImageCache: ref=docker-cache
2022-04-05T15:31:17.821+0100 [ERROR] 2022-04-05T15:31:17.821+0100 [DEBUG] Connecting cache to network: name=network.dc1
2022-04-05T15:31:17.822+0100 [ERROR] 2022-04-05T15:31:17.822+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-05T15:31:17.839+0100 [ERROR] 2022-04-05T15:31:17.839+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-05T15:31:17.839+0100 [DEBUG] Creating Docker Container: ref=39412348-import
2022-04-05T15:31:20.381+0100 [ERROR] 2022-04-05T15:31:20.381+0100 [DEBUG] Forcefully remove: container=b82c9801aaf8551ca43ea5519e143cb1733eb171de471fd60fa0eccdc05c6d8a
2022-04-05T15:31:20.800+0100 [ERROR] 2022-04-05T15:31:20.799+0100 [DEBUG] Image exists in local cache: image=shipyardrun/docker-registry-proxy:0.6.3
2022-04-05T15:31:20.800+0100 [ERROR] 2022-04-05T15:31:20.800+0100 [DEBUG] Creating Docker Container: ref=docker-cache
2022-04-05T15:31:20.849+0100 [ERROR] 2022-04-05T15:31:20.849+0100 [DEBUG] Remove container from default networks: ref=docker-cache
2022-04-05T15:31:20.852+0100 [ERROR] 2022-04-05T15:31:20.852+0100 [DEBUG] Attaching container to network: ref=bbfc08c1ee6527702e40ea18867e2515502e5fd5fe0b03f322358ee633b413ef network=dc1
2022-04-05T15:31:20.858+0100 [ERROR] 2022-04-05T15:31:20.858+0100 [DEBUG] Disconnectng network: name=bridge ref=docker-cache
2022-04-05T15:31:21.389+0100 [ERROR] 2022-04-05T15:31:21.388+0100 [INFO]  dc1: Creating Cluster: ref=dc1
2022-04-05T15:31:21.411+0100 [ERROR] 2022-04-05T15:31:21.411+0100 [DEBUG] Image exists in local cache: image=shipyardrun/k3s:v1.22.4
2022-04-05T15:31:21.412+0100 [ERROR] 2022-04-05T15:31:21.412+0100 [DEBUG] Volume exists: ref=images name=images.volume.shipyard.run
2022-04-05T15:31:21.412+0100 [ERROR] 2022-04-05T15:31:21.412+0100 [DEBUG] Creating Docker Container: ref=server.dc1
2022-04-05T15:31:21.463+0100 [ERROR] 2022-04-05T15:31:21.463+0100 [DEBUG] Remove container from default networks: ref=server.dc1
2022-04-05T15:31:21.467+0100 [ERROR] 2022-04-05T15:31:21.467+0100 [DEBUG] Attaching container to network: ref=bfe28d4312bd0a621bb97b4c72b41e3d1318d70bc8c06d08501b9c69e365d53f network=dc1
2022-04-05T15:31:21.476+0100 [ERROR] 2022-04-05T15:31:21.476+0100 [DEBUG] Disconnectng network: name=bridge ref=server.dc1
2022-04-05T15:31:24.067+0100 [ERROR] 2022-04-05T15:31:24.067+0100 [DEBUG] Copying file from: id=bfe28d4312bd0a621bb97b4c72b41e3d1318d70bc8c06d08501b9c69e365d53f src=/output/kubeconfig.yaml dst=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:31:24.104+0100 [ERROR] 2022-04-05T15:31:24.104+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:26.113+0100 [ERROR] 2022-04-05T15:31:26.113+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:28.116+0100 [ERROR] 2022-04-05T15:31:28.116+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:29.644+0100 [ERROR] 2022-04-05T15:31:29.644+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 15.000652]
2022-04-05T15:31:30.119+0100 [ERROR] 2022-04-05T15:31:30.119+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:32.123+0100 [ERROR] 2022-04-05T15:31:32.123+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:34.127+0100 [ERROR] 2022-04-05T15:31:34.127+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:36.131+0100 [ERROR] 2022-04-05T15:31:36.130+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:38.133+0100 [ERROR] 2022-04-05T15:31:38.133+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:40.137+0100 [ERROR] 2022-04-05T15:31:40.137+0100 [DEBUG] Less than one item returned, will retry: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:42.142+0100 [ERROR] 2022-04-05T15:31:42.141+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=local-path-provisioner-64ffb68fd-hlswc namespace=kube-system status=Pending
2022-04-05T15:31:44.146+0100 [ERROR] 2022-04-05T15:31:44.146+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=local-path-provisioner
2022-04-05T15:31:44.146+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-05T15:31:44.644+0100 [ERROR] 2022-04-05T15:31:44.644+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 30.000369]
2022-04-05T15:31:46.149+0100 [ERROR] 2022-04-05T15:31:46.149+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=k8s-app=kube-dns
2022-04-05T15:31:46.149+0100 [DEBUG] Writing docker images to volume: images=[] volume=images.volume.shipyard.run
2022-04-05T15:31:46.166+0100 [ERROR] 2022-04-05T15:31:46.166+0100 [DEBUG] Image exists in local cache: image=alpine:latest
2022-04-05T15:31:46.166+0100 [DEBUG] Creating Docker Container: ref=66538416-import
2022-04-05T15:31:48.639+0100 [ERROR] 2022-04-05T15:31:48.639+0100 [DEBUG] Forcefully remove: container=51d1374847f4bc317ba4ca6fbd5f1d40ec432d810fd233878608994297c1e2a2
2022-04-05T15:31:48.959+0100 [ERROR] 2022-04-05T15:31:48.959+0100 [DEBUG] dc1: Deploying connector
2022-04-05T15:31:50.340+0100 [ERROR] 2022-04-05T15:31:50.340+0100 [DEBUG] dc1: Writing namespace config: file=/tmp/2811884817/namespace.yaml
2022-04-05T15:31:50.340+0100 [DEBUG] dc1: Writing secret config: file=/tmp/2811884817/secret.yaml
2022-04-05T15:31:50.340+0100 [ERROR] 2022-04-05T15:31:50.340+0100 [DEBUG] dc1: Writing RBAC config: file=/tmp/2811884817/rbac.yaml
2022-04-05T15:31:50.340+0100 [ERROR] 2022-04-05T15:31:50.340+0100 [DEBUG] dc1: Writing deployment config: file=/tmp/2811884817/deployment.yaml
2022-04-05T15:31:50.340+0100 [ERROR] 2022-04-05T15:31:50.340+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2811884817/namespace.yaml
2022-04-05T15:31:50.862+0100 [ERROR] 2022-04-05T15:31:50.862+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2811884817/secret.yaml
2022-04-05T15:31:50.867+0100 [ERROR] 2022-04-05T15:31:50.867+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2811884817/rbac.yaml
2022-04-05T15:31:50.874+0100 [ERROR] 2022-04-05T15:31:50.874+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/tmp/2811884817/deployment.yaml
2022-04-05T15:31:50.890+0100 [ERROR] 2022-04-05T15:31:50.890+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-05T15:31:52.893+0100 [ERROR] 2022-04-05T15:31:52.893+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app=connector
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=web
2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=consul-rpc
2022-04-05T15:31:52.894+0100 [INFO]  Applying Kubernetes configuration: ref=consul_namespace config=["/home/nicj/.shipyard/data/consul/namespace.yaml"]
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=upstreams-proxy
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=web local_port=9092 connector_addr=127.0.0.1:31246 local_addr=web.default.svc:9090
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=upstreams-proxy local_port=28080 connector_addr=127.0.0.1:31246 local_addr=consul-release-controller.default.svc:8080
2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-1
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=controller-webhook
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=consul-ingeress-gateway-2
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=consul-lan-serf
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=consul-lan-serf local_port=8301 connector_addr=127.0.0.1:31246 local_addr=consul-server.consul.svc:8301
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=consul-rpc local_port=8300 connector_addr=127.0.0.1:31246 local_addr=consul-server.consul.svc:8300
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-1 local_port=18080 connector_addr=127.0.0.1:31246 local_addr=consul-ingress-gateway.consul.svc:18080
2022-04-05T15:31:52.894+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul/namespace.yaml
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [INFO]  Applying Kubernetes configuration: ref=cert-manager-controller config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml"]
2022-04-05T15:31:52.894+0100 [INFO]  Create Ingress: ref=consul
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=consul local_port=8501 connector_addr=127.0.0.1:31246 local_addr=consul-server.consul.svc:8501
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose local service: name=controller-webhook remote_port=19443 connector_addr=127.0.0.1:31246 local_addr=localhost:19443
2022-04-05T15:31:52.894+0100 [ERROR] 2022-04-05T15:31:52.894+0100 [DEBUG] Calling connector to expose remote service: name=consul-ingeress-gateway-2 local_port=18443 connector_addr=127.0.0.1:31246 local_addr=consul-ingress-gateway.consul.svc:18443
2022-04-05T15:31:52.895+0100 [ERROR] 2022-04-05T15:31:52.895+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/cert-manager.yaml
2022-04-05T15:31:52.914+0100 [ERROR] 2022-04-05T15:31:52.914+0100 [DEBUG] Successfully exposed service: id=80a9f5b7-2146-4fa1-ba43-90653767e3c1
2022-04-05T15:31:52.915+0100 [ERROR] 2022-04-05T15:31:52.915+0100 [DEBUG] Successfully exposed service: id=2e970413-132b-43c3-9207-719a118ab9f4
2022-04-05T15:31:52.916+0100 [ERROR] 2022-04-05T15:31:52.916+0100 [DEBUG] Successfully exposed service: id=e9882f55-1b26-4797-9db1-225a5aae969f
2022-04-05T15:31:52.918+0100 [ERROR] 2022-04-05T15:31:52.918+0100 [DEBUG] Successfully exposed service: id=1075ce46-24c5-4cc5-a723-d1240b4a6307
2022-04-05T15:31:52.918+0100 [ERROR] 2022-04-05T15:31:52.918+0100 [DEBUG] Successfully exposed service: id=27c4bf68-b92d-47c2-83ee-d8ca4cf6d790
2022-04-05T15:31:52.919+0100 [ERROR] 2022-04-05T15:31:52.919+0100 [DEBUG] Successfully exposed service: id=963f10b5-6dad-4fe3-b58e-2a9b9506ad6b
2022-04-05T15:31:52.921+0100 [ERROR] 2022-04-05T15:31:52.921+0100 [DEBUG] Successfully exposed service: id=5da0479b-7375-4d0c-8d65-437475656100
2022-04-05T15:31:52.922+0100 [ERROR] 2022-04-05T15:31:52.922+0100 [DEBUG] Successfully exposed service: id=dacf1cb6-ba64-45d4-91b0-8da9af742e83
2022-04-05T15:31:52.960+0100 [ERROR] 2022-04-05T15:31:52.960+0100 [INFO]  Creating Helm chart: ref=consul
2022-04-05T15:31:52.960+0100 [DEBUG] Updating Helm chart repository: name=hashicorp url=https://helm.releases.hashicorp.com
2022-04-05T15:31:53.054+0100 [ERROR] 2022-04-05T15:31:53.054+0100 [DEBUG] Using Kubernetes config: ref=consul path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:31:53.055+0100 [ERROR] 2022-04-05T15:31:53.054+0100 [DEBUG] Creating chart from config: ref=consul chart=hashicorp/consul
2022-04-05T15:31:53.150+0100 [ERROR] 2022-04-05T15:31:53.150+0100 [DEBUG] Loading chart: ref=consul path=/home/nicj/.shipyard/helm_charts/cache/consul-0.40.0.tgz
2022-04-05T15:31:53.157+0100 [ERROR] 2022-04-05T15:31:53.157+0100 [DEBUG] Using Values: ref=consul
  values=
  | map[connectInject:map[centralConfig:map[enabled:true] default:false enabled:true envoyExtraArgs:<nil> failurePolicy:Ignore replicas:1 transparentProxy:map[defaultEnabled:false]] controller:map[enabled:true] global:map[acls:map[manageSystemACLs:true] datacenter:dc1 federation:map[createFederationSecret:false enabled:false] image:hashicorp/consul:1.11.3 imageEnvoy:envoyproxy/envoy:v1.20.1 imageK8S:hashicorp/consul-k8s-control-plane:0.40.0 logLevel:info metrics:map[enableAgentMetrics:true enableGatewayMetrics:true enabled:true] name:consul tls:map[enableAutoEncrypt:true enabled:true httpsOnly:false]] ingressGateways:map[defaults:map[replicas:1 service:map[ports:[map[nodePort:<nil> port:18080] map[nodePort:<nil> port:18443]]]] enabled:true] meshGateway:map[enabled:false replicas:1 service:map[enabled:false nodePort:30443 type:NodePort] wanAddress:map[port:30443 source:Static static:dc1.k8s-cluster.shipyard.run]] server:map[bootstrapExpect:1 extraConfig:{
  |   "ui_config": {
  |     "enabled": true,
  |     "metrics_provider": "prometheus",
  |     "metrics_proxy": {
  |       "base_url": "http://prometheus-kube-prometheus-prometheus.monitoring.svc:9090"
  |     }
  |   }
  | }
  |  replicas:1 storage:128Mi] ui:map[enabled:true]]
  
2022-04-05T15:31:53.157+0100 [DEBUG] Validate chart: ref=consul
2022-04-05T15:31:53.157+0100 [DEBUG] Run chart: ref=consul
2022-04-05T15:31:53.294+0100 [ERROR] 2022-04-05T15:31:53.294+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-05T15:31:53.858+0100 [ERROR] 2022-04-05T15:31:53.858+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" ServiceAccount"
2022-04-05T15:31:53.860+0100 [ERROR] 2022-04-05T15:31:53.860+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="serviceaccounts \"consul-tls-init\" not found"
2022-04-05T15:31:53.911+0100 [ERROR] 2022-04-05T15:31:53.910+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-05T15:31:53.915+0100 [ERROR] 2022-04-05T15:31:53.915+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Role"
2022-04-05T15:31:53.917+0100 [ERROR] 2022-04-05T15:31:53.917+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="roles.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-05T15:31:53.980+0100 [ERROR] 2022-04-05T15:31:53.980+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-05T15:31:53.984+0100 [ERROR] 2022-04-05T15:31:53.984+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" RoleBinding"
2022-04-05T15:31:53.986+0100 [ERROR] 2022-04-05T15:31:53.986+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="rolebindings.rbac.authorization.k8s.io \"consul-tls-init\" not found"
2022-04-05T15:31:54.036+0100 [ERROR] 2022-04-05T15:31:54.035+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-05T15:31:54.040+0100 [ERROR] 2022-04-05T15:31:54.040+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-05T15:31:54.042+0100 [ERROR] 2022-04-05T15:31:54.042+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="jobs.batch \"consul-tls-init\" not found"
2022-04-05T15:31:54.091+0100 [ERROR] 2022-04-05T15:31:54.091+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-05T15:31:54.097+0100 [ERROR] 2022-04-05T15:31:54.097+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-tls-init with timeout of 0s"
2022-04-05T15:31:54.102+0100 [ERROR] 2022-04-05T15:31:54.101+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: ADDED"
2022-04-05T15:31:54.102+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:31:54.111+0100 [ERROR] 2022-04-05T15:31:54.111+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-05T15:31:54.111+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:31:55.298+0100 [ERROR] 2022-04-05T15:31:55.298+0100 [DEBUG] Pod not running: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-cainjector-7974c84449-cb87l namespace=cert-manager status=Pending
2022-04-05T15:31:56.762+0100 [ERROR] 2022-04-05T15:31:56.762+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-05T15:31:56.762+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-tls-init: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:31:56.770+0100 [ERROR] 2022-04-05T15:31:56.770+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-tls-init: MODIFIED"
2022-04-05T15:31:56.772+0100 [ERROR] 2022-04-05T15:31:56.772+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-tls-init\" Job"
2022-04-05T15:31:56.780+0100 [ERROR] 2022-04-05T15:31:56.780+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 58 resource(s)"
2022-04-05T15:31:57.192+0100 [ERROR] 2022-04-05T15:31:57.191+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="creating 1 resource(s)"
2022-04-05T15:31:57.206+0100 [ERROR] 2022-04-05T15:31:57.206+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Watching for changes to Job consul-server-acl-init-cleanup with timeout of 0s"
2022-04-05T15:31:57.208+0100 [ERROR] 2022-04-05T15:31:57.208+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: ADDED"
2022-04-05T15:31:57.208+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:31:57.214+0100 [ERROR] 2022-04-05T15:31:57.214+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-05T15:31:57.214+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:31:57.304+0100 [ERROR] 2022-04-05T15:31:57.303+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-q8jtt namespace=cert-manager type=Ready value=False
2022-04-05T15:31:59.308+0100 [ERROR] 2022-04-05T15:31:59.308+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-q8jtt namespace=cert-manager type=Ready value=False
2022-04-05T15:31:59.645+0100 [ERROR] 2022-04-05T15:31:59.645+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 45.001100]
2022-04-05T15:32:01.325+0100 [ERROR] 2022-04-05T15:32:01.325+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-q8jtt namespace=cert-manager type=Ready value=False
2022-04-05T15:32:03.330+0100 [ERROR] 2022-04-05T15:32:03.330+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-q8jtt namespace=cert-manager type=Ready value=False
2022-04-05T15:32:05.335+0100 [ERROR] 2022-04-05T15:32:05.335+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-q8jtt namespace=cert-manager type=Ready value=False
2022-04-05T15:32:07.340+0100 [ERROR] 2022-04-05T15:32:07.340+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=cert-manager-webhook-59d6cfd784-q8jtt namespace=cert-manager type=Ready value=False
2022-04-05T15:32:09.345+0100 [ERROR] 2022-04-05T15:32:09.345+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=app.kubernetes.io/instance=cert-manager
2022-04-05T15:32:14.645+0100 [ERROR] 2022-04-05T15:32:14.645+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 60.001718]
2022-04-05T15:32:20.937+0100 [ERROR] 2022-04-05T15:32:20.937+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-05T15:32:20.937+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="consul-server-acl-init-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:20.945+0100 [ERROR] 2022-04-05T15:32:20.945+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Add/Modify event for consul-server-acl-init-cleanup: MODIFIED"
2022-04-05T15:32:20.947+0100 [ERROR] 2022-04-05T15:32:20.947+0100 [DEBUG] Helm debug: name=consul chart=hashicorp/consul message="Starting delete for \"consul-server-acl-init-cleanup\" Job"
2022-04-05T15:32:21.005+0100 [ERROR] 2022-04-05T15:32:21.005+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-05T15:32:23.010+0100 [ERROR] 2022-04-05T15:32:23.010+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=consul-connect-injector-57d85f9c7c-wjrlr namespace=consul type=Ready value=False
2022-04-05T15:32:25.016+0100 [ERROR] 2022-04-05T15:32:25.016+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=connect-injector
2022-04-05T15:32:25.016+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-05T15:32:27.021+0100 [ERROR] 2022-04-05T15:32:27.021+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=client
2022-04-05T15:32:27.021+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-05T15:32:29.027+0100 [ERROR] 2022-04-05T15:32:29.027+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=controller
2022-04-05T15:32:29.027+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-05T15:32:29.644+0100 [ERROR] 2022-04-05T15:32:29.644+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 75.000414]
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=component=server
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Generating template: ref=prometheus_operator_template output=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-05T15:32:31.032+0100 [DEBUG] Template content: ref=prometheus_operator_template
  source=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: #{{ .Vars.monitoring_namespace }}
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
  
2022-04-05T15:32:31.032+0100 [INFO]  Applying Kubernetes configuration: ref=consul_defaults config=["/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml"]
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Create Ingress: ref=grafana
2022-04-05T15:32:31.032+0100 [INFO]  Create Ingress: ref=prometheus
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Generating template: ref=monitoring_namespace output=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-05T15:32:31.032+0100 [DEBUG] Template content: ref=monitoring_namespace
  source=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Calling connector to expose remote service: name=prometheus local_port=9090 connector_addr=127.0.0.1:31246 local_addr=prometheus-operated.monitoring.svc:9090
2022-04-05T15:32:31.032+0100 [DEBUG] Calling connector to expose remote service: name=grafana local_port=8080 connector_addr=127.0.0.1:31246 local_addr=grafana.monitoring.svc:80
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Template output: ref=prometheus_operator_template
  destination=
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   jobLabel: applications
  |   endpoints:
  |   - port: metrics
  |     interval: 15s
  |   namespaceSelector:
  |     matchNames:
  |     - default
  | 
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: PodMonitor
  | metadata:
  |   name: applications
  |   namespace: monitoring
  |   labels:
  |     app: applications
  |     release: prometheus
  | spec:
  |   selector:
  |     matchLabels:
  |       metrics: enabled
  |   podMetricsEndpoints:
  |   - port: "9102"
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Template output: ref=monitoring_namespace
  destination=
  |   kind: Namespace
  |   apiVersion: v1
  |   metadata:
  |     name: monitoring
  |     labels:
  |       name: monitoring
  
2022-04-05T15:32:31.032+0100 [INFO]  Create Ingress: ref=zipkin
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Generating template: ref=fetch_consul_resources output=/home/nicj/.shipyard/data/consul_kubernetes/fetch.sh
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Template content: ref=fetch_consul_resources
  source=
  |   #!/bin/sh -e
  | 
  |   echo "Port #{{ .Vars.port }}"
  |   echo "Fetching resources from running cluster, acls_enabled: #{{ .Vars.acl_enabled }}, tls_enabled #{{ .Vars.tls_enabled }}"
  | 
  |   #{{ if eq .Vars.acl_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   #{{end}}
  |   
  |   #{{ if eq .Vars.tls_enabled true }}
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n #{{ .Vars.consul_namespace }} -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |   #{{end}}
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Creating Helm chart: ref=prometheus
2022-04-05T15:32:31.032+0100 [INFO]  Create Ingress: ref=tempo
2022-04-05T15:32:31.032+0100 [DEBUG] Updating Helm chart repository: name=prometheus url=https://prometheus-community.github.io/helm-charts
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Calling connector to expose remote service: name=tempo local_port=3100 connector_addr=127.0.0.1:31246 local_addr=tempo.default.svc:3100
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Applying Kubernetes configuration: ref=monitoring_namespace config=["/home/nicj/.shipyard/data/monitoring/namespace.yaml"]
2022-04-05T15:32:31.032+0100 [DEBUG] Template output: ref=fetch_consul_resources
  destination=
  |   #!/bin/sh -e
  | 
  |   echo "Port 8501"
  |   echo "Fetching resources from running cluster, acls_enabled: true, tls_enabled true"
  | 
  |   
  |   kubectl get secret -n consul -o jsonpath='{.data.token}' consul-bootstrap-acl-token | base64 -d > /data/bootstrap_acl.token
  |   
  |   
  |   
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.crt']}" consul-ca-cert | base64 -d > /data/tls.crt
  |   kubectl get secret -n consul -o jsonpath="{.data['tls\.key']}" consul-ca-key | base64 -d > /data/tls.key
  |
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Remote executing command: ref=fetch_consul_resources command=sh args=["/data/fetch.sh"] image="&{shipyardrun/tools:v0.5.0  }"
2022-04-05T15:32:31.032+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/proxy-defaults.yaml
2022-04-05T15:32:31.032+0100 [DEBUG] Calling connector to expose remote service: name=zipkin local_port=9411 connector_addr=127.0.0.1:31246 local_addr=tempo.monitoring.svc:9411
2022-04-05T15:32:31.033+0100 [ERROR] 2022-04-05T15:32:31.032+0100 [INFO]  Generating template: ref=grafana_secret_template output=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-05T15:32:31.033+0100 [DEBUG] Template content: ref=grafana_secret_template
  source=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
2022-04-05T15:32:31.033+0100 [ERROR] 2022-04-05T15:32:31.033+0100 [DEBUG] Template output: ref=grafana_secret_template
  destination=
  | apiVersion: v1
  | kind: Secret
  | metadata:
  |   name: grafana-password
  |   namespace: monitoring
  | type: Opaque
  | data:
  |   admin-password: YWRtaW4=
  |   admin-user: YWRtaW4=
2022-04-05T15:32:31.033+0100 [ERROR] 2022-04-05T15:32:31.033+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/namespace.yaml
2022-04-05T15:32:31.051+0100 [ERROR] 2022-04-05T15:32:31.051+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.5.0
2022-04-05T15:32:31.051+0100 [DEBUG] Creating Docker Container: ref=fetch_consul_resources.remote_exec
2022-04-05T15:32:31.052+0100 [ERROR] 2022-04-05T15:32:31.052+0100 [DEBUG] Successfully exposed service: id=2f9e3954-406d-47c8-80ed-e44962058d62
2022-04-05T15:32:31.059+0100 [ERROR] 2022-04-05T15:32:31.059+0100 [DEBUG] Successfully exposed service: id=7c23e088-ee1c-469c-bfc2-3297e1aaa0e4
2022-04-05T15:32:31.059+0100 [ERROR] 2022-04-05T15:32:31.059+0100 [DEBUG] Successfully exposed service: id=68ee8606-a5e9-4af9-ae3c-515ba6bc8ffa
2022-04-05T15:32:31.060+0100 [ERROR] 2022-04-05T15:32:31.060+0100 [DEBUG] Successfully exposed service: id=76ca1c24-743d-4a43-a588-ff5c0acf25fe
2022-04-05T15:32:31.104+0100 [ERROR] 2022-04-05T15:32:31.104+0100 [DEBUG] Remove container from default networks: ref=fetch_consul_resources.remote_exec
2022-04-05T15:32:31.107+0100 [ERROR] 2022-04-05T15:32:31.107+0100 [DEBUG] Attaching container to network: ref=380ee5bda04929cade10544d9994251c3ed5a8b96c37fe65b349c1af79db229f network=dc1
2022-04-05T15:32:31.112+0100 [ERROR] 2022-04-05T15:32:31.112+0100 [DEBUG] Disconnectng network: name=bridge ref=fetch_consul_resources.remote_exec
2022-04-05T15:32:31.427+0100 [ERROR] 2022-04-05T15:32:31.427+0100 [DEBUG] Using Kubernetes config: ref=prometheus path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:32:31.428+0100 [ERROR] 2022-04-05T15:32:31.428+0100 [DEBUG] Creating chart from config: ref=prometheus chart=prometheus/kube-prometheus-stack
2022-04-05T15:32:31.583+0100 [ERROR] 2022-04-05T15:32:31.583+0100 [INFO]  Applying Kubernetes configuration: ref=grafana_secret config=["/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml"]
2022-04-05T15:32:31.584+0100 [ERROR] 2022-04-05T15:32:31.584+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/grafana_secret.yaml
2022-04-05T15:32:31.844+0100 [ERROR] 2022-04-05T15:32:31.844+0100 [DEBUG] Port 8501
Fetching resources from running cluster, acls_enabled: true, tls_enabled true
2022-04-05T15:32:32.160+0100 [ERROR] 2022-04-05T15:32:32.160+0100 [DEBUG] Forcefully remove: container=380ee5bda04929cade10544d9994251c3ed5a8b96c37fe65b349c1af79db229f
2022-04-05T15:32:32.831+0100 [ERROR] 2022-04-05T15:32:32.831+0100 [DEBUG] Loading chart: ref=prometheus path=/home/nicj/.shipyard/helm_charts/cache/kube-prometheus-stack-32.0.0.tgz
2022-04-05T15:32:32.844+0100 [ERROR] 2022-04-05T15:32:32.844+0100 [DEBUG] Using Values: ref=prometheus values="map[alertmanager:map[enabled:false] defaultRules:map[create:false] grafana:map[enabled:false] serviceMonitor:map[enabled:false]]"
2022-04-05T15:32:32.844+0100 [DEBUG] Validate chart: ref=prometheus
2022-04-05T15:32:32.844+0100 [DEBUG] Run chart: ref=prometheus
2022-04-05T15:32:32.858+0100 [ERROR] 2022-04-05T15:32:32.858+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:32.905+0100 [ERROR] 2022-04-05T15:32:32.904+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:32.961+0100 [ERROR] 2022-04-05T15:32:32.961+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:32.972+0100 [ERROR] 2022-04-05T15:32:32.972+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:33.007+0100 [ERROR] 2022-04-05T15:32:33.006+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:33.067+0100 [ERROR] 2022-04-05T15:32:33.067+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:33.079+0100 [ERROR] 2022-04-05T15:32:33.079+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:33.125+0100 [ERROR] 2022-04-05T15:32:33.125+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:33.165+0100 [ERROR] 2022-04-05T15:32:33.165+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Clearing discovery cache"
2022-04-05T15:32:33.165+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="beginning wait for 8 resources with timeout of 1m0s"
2022-04-05T15:32:36.542+0100 [ERROR] 2022-04-05T15:32:36.541+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:36.818+0100 [ERROR] 2022-04-05T15:32:36.818+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-05T15:32:36.821+0100 [ERROR] 2022-04-05T15:32:36.821+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:37.107+0100 [ERROR] 2022-04-05T15:32:37.107+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:37.113+0100 [ERROR] 2022-04-05T15:32:37.113+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-05T15:32:37.116+0100 [ERROR] 2022-04-05T15:32:37.116+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:37.389+0100 [ERROR] 2022-04-05T15:32:37.389+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:37.394+0100 [ERROR] 2022-04-05T15:32:37.394+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-05T15:32:37.397+0100 [ERROR] 2022-04-05T15:32:37.397+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:37.678+0100 [ERROR] 2022-04-05T15:32:37.678+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:37.683+0100 [ERROR] 2022-04-05T15:32:37.683+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-05T15:32:37.685+0100 [ERROR] 2022-04-05T15:32:37.685+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:37.963+0100 [ERROR] 2022-04-05T15:32:37.963+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:37.969+0100 [ERROR] 2022-04-05T15:32:37.969+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-05T15:32:37.971+0100 [ERROR] 2022-04-05T15:32:37.971+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:38.241+0100 [ERROR] 2022-04-05T15:32:38.240+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:38.245+0100 [ERROR] 2022-04-05T15:32:38.245+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-05T15:32:38.247+0100 [ERROR] 2022-04-05T15:32:38.247+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-create\" not found"
2022-04-05T15:32:38.520+0100 [ERROR] 2022-04-05T15:32:38.520+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:38.524+0100 [ERROR] 2022-04-05T15:32:38.524+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-create with timeout of 0s"
2022-04-05T15:32:38.526+0100 [ERROR] 2022-04-05T15:32:38.526+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: ADDED"
2022-04-05T15:32:38.526+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:38.543+0100 [ERROR] 2022-04-05T15:32:38.543+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-05T15:32:38.543+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:40.880+0100 [ERROR] 2022-04-05T15:32:40.880+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-05T15:32:40.880+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-create: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:40.891+0100 [ERROR] 2022-04-05T15:32:40.891+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-create: MODIFIED"
2022-04-05T15:32:40.893+0100 [ERROR] 2022-04-05T15:32:40.893+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-05T15:32:40.902+0100 [ERROR] 2022-04-05T15:32:40.902+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-05T15:32:40.913+0100 [ERROR] 2022-04-05T15:32:40.913+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-05T15:32:40.918+0100 [ERROR] 2022-04-05T15:32:40.918+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-05T15:32:40.922+0100 [ERROR] 2022-04-05T15:32:40.922+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-05T15:32:40.927+0100 [ERROR] 2022-04-05T15:32:40.927+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-create\" Job"
2022-04-05T15:32:40.930+0100 [ERROR] 2022-04-05T15:32:40.930+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 36 resource(s)"
2022-04-05T15:32:41.121+0100 [ERROR] 2022-04-05T15:32:41.121+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-05T15:32:41.125+0100 [ERROR] 2022-04-05T15:32:41.125+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="serviceaccounts \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:41.404+0100 [ERROR] 2022-04-05T15:32:41.404+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:41.409+0100 [ERROR] 2022-04-05T15:32:41.409+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-05T15:32:41.411+0100 [ERROR] 2022-04-05T15:32:41.411+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterroles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:41.686+0100 [ERROR] 2022-04-05T15:32:41.686+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:41.691+0100 [ERROR] 2022-04-05T15:32:41.691+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-05T15:32:41.694+0100 [ERROR] 2022-04-05T15:32:41.694+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="clusterrolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:41.994+0100 [ERROR] 2022-04-05T15:32:41.994+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:41.999+0100 [ERROR] 2022-04-05T15:32:41.999+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-05T15:32:42.001+0100 [ERROR] 2022-04-05T15:32:42.001+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="roles.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:42.290+0100 [ERROR] 2022-04-05T15:32:42.290+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:42.296+0100 [ERROR] 2022-04-05T15:32:42.296+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-05T15:32:42.299+0100 [ERROR] 2022-04-05T15:32:42.299+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="rolebindings.rbac.authorization.k8s.io \"prometheus-kube-prometheus-admission\" not found"
2022-04-05T15:32:42.611+0100 [ERROR] 2022-04-05T15:32:42.611+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:42.616+0100 [ERROR] 2022-04-05T15:32:42.616+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-05T15:32:42.619+0100 [ERROR] 2022-04-05T15:32:42.619+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="jobs.batch \"prometheus-kube-prometheus-admission-patch\" not found"
2022-04-05T15:32:42.906+0100 [ERROR] 2022-04-05T15:32:42.906+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="creating 1 resource(s)"
2022-04-05T15:32:42.909+0100 [ERROR] 2022-04-05T15:32:42.909+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Watching for changes to Job prometheus-kube-prometheus-admission-patch with timeout of 0s"
2022-04-05T15:32:42.911+0100 [ERROR] 2022-04-05T15:32:42.911+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: ADDED"
2022-04-05T15:32:42.911+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:42.922+0100 [ERROR] 2022-04-05T15:32:42.922+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-05T15:32:42.922+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 1, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:44.645+0100 [ERROR] 2022-04-05T15:32:44.645+0100 [INFO]  Please wait, still creating resources [Elapsed Time: 90.000930]
2022-04-05T15:32:45.881+0100 [ERROR] 2022-04-05T15:32:45.881+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-05T15:32:45.881+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="prometheus-kube-prometheus-admission-patch: Jobs active: 0, jobs failed: 0, jobs succeeded: 0"
2022-04-05T15:32:45.889+0100 [ERROR] 2022-04-05T15:32:45.889+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Add/Modify event for prometheus-kube-prometheus-admission-patch: MODIFIED"
2022-04-05T15:32:45.891+0100 [ERROR] 2022-04-05T15:32:45.891+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ServiceAccount"
2022-04-05T15:32:45.898+0100 [ERROR] 2022-04-05T15:32:45.898+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRole"
2022-04-05T15:32:45.906+0100 [ERROR] 2022-04-05T15:32:45.906+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" ClusterRoleBinding"
2022-04-05T15:32:45.911+0100 [ERROR] 2022-04-05T15:32:45.911+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" Role"
2022-04-05T15:32:45.916+0100 [ERROR] 2022-04-05T15:32:45.916+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission\" RoleBinding"
2022-04-05T15:32:45.921+0100 [ERROR] 2022-04-05T15:32:45.921+0100 [DEBUG] Helm debug: name=prometheus chart=prometheus/kube-prometheus-stack message="Starting delete for \"prometheus-kube-prometheus-admission-patch\" Job"
2022-04-05T15:32:46.202+0100 [ERROR] 2022-04-05T15:32:46.201+0100 [DEBUG] Health checking pods: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-05T15:32:48.207+0100 [ERROR] 2022-04-05T15:32:48.207+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-stjqq namespace=monitoring type=Ready value=False
2022-04-05T15:32:50.213+0100 [ERROR] 2022-04-05T15:32:50.213+0100 [DEBUG] Pod not ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml pod=prometheus-kube-state-metrics-57c988498f-stjqq namespace=monitoring type=Ready value=False
2022-04-05T15:32:52.218+0100 [ERROR] 2022-04-05T15:32:52.218+0100 [DEBUG] Pods ready: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml selector=release=prometheus
2022-04-05T15:32:52.218+0100 [ERROR] 2022-04-05T15:32:52.218+0100 [INFO]  Applying Kubernetes configuration: ref=prometheus config=["/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml"]
2022-04-05T15:32:52.218+0100 [INFO]  Creating Helm chart: ref=loki
2022-04-05T15:32:52.218+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-05T15:32:52.219+0100 [ERROR] 2022-04-05T15:32:52.219+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/monitoring/prometheus_operator.yaml
2022-04-05T15:32:52.432+0100 [ERROR] 2022-04-05T15:32:52.432+0100 [DEBUG] Using Kubernetes config: ref=loki path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:32:52.432+0100 [ERROR] 2022-04-05T15:32:52.432+0100 [DEBUG] Creating chart from config: ref=loki chart=grafana/loki
2022-04-05T15:32:53.117+0100 [ERROR] 2022-04-05T15:32:53.117+0100 [DEBUG] Loading chart: ref=loki path=/home/nicj/.shipyard/helm_charts/cache/loki-2.9.1.tgz
2022-04-05T15:32:53.118+0100 [ERROR] 2022-04-05T15:32:53.118+0100 [DEBUG] Using Values: ref=loki values=map[]
2022-04-05T15:32:53.118+0100 [DEBUG] Validate chart: ref=loki
2022-04-05T15:32:53.118+0100 [DEBUG] Run chart: ref=loki
2022-04-05T15:32:53.352+0100 [ERROR] W0405 15:32:53.352927   25671 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-05T15:32:53.368+0100 [ERROR] 2022-04-05T15:32:53.368+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 1 resource(s)"
2022-04-05T15:32:53.377+0100 [ERROR] 2022-04-05T15:32:53.377+0100 [DEBUG] Helm debug: name=loki chart=grafana/loki message="creating 8 resource(s)"
2022-04-05T15:32:53.381+0100 [ERROR] W0405 15:32:53.381316   25671 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-05T15:32:53.412+0100 [ERROR] 2022-04-05T15:32:53.412+0100 [INFO]  Creating Helm chart: ref=promtail
2022-04-05T15:32:53.412+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-05T15:32:53.412+0100 [DEBUG] Using Kubernetes config: ref=promtail path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:32:53.412+0100 [ERROR] 2022-04-05T15:32:53.412+0100 [DEBUG] Creating chart from config: ref=promtail chart=grafana/promtail
2022-04-05T15:32:53.765+0100 [ERROR] 2022-04-05T15:32:53.765+0100 [DEBUG] Loading chart: ref=promtail path=/home/nicj/.shipyard/helm_charts/cache/promtail-3.11.0.tgz
2022-04-05T15:32:53.766+0100 [ERROR] 2022-04-05T15:32:53.766+0100 [DEBUG] Using Values: ref=promtail values=map[config:map[lokiAddress:http://loki:3100/loki/api/v1/push]]
2022-04-05T15:32:53.766+0100 [DEBUG] Validate chart: ref=promtail
2022-04-05T15:32:53.766+0100 [DEBUG] Run chart: ref=promtail
2022-04-05T15:32:54.015+0100 [ERROR] 2022-04-05T15:32:54.015+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 1 resource(s)"
2022-04-05T15:32:54.024+0100 [ERROR] 2022-04-05T15:32:54.023+0100 [DEBUG] Helm debug: name=promtail chart=grafana/promtail message="creating 5 resource(s)"
2022-04-05T15:32:54.049+0100 [ERROR] 2022-04-05T15:32:54.049+0100 [INFO]  Creating Helm chart: ref=tempo
2022-04-05T15:32:54.049+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-05T15:32:54.049+0100 [DEBUG] Using Kubernetes config: ref=tempo path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:32:54.050+0100 [ERROR] 2022-04-05T15:32:54.050+0100 [DEBUG] Creating chart from config: ref=tempo chart=grafana/tempo
2022-04-05T15:32:54.656+0100 [ERROR] 2022-04-05T15:32:54.656+0100 [DEBUG] Loading chart: ref=tempo path=/home/nicj/.shipyard/helm_charts/cache/tempo-0.13.1.tgz
2022-04-05T15:32:54.656+0100 [ERROR] 2022-04-05T15:32:54.656+0100 [DEBUG] Using Values: ref=tempo values="map[tempo:map[receivers:map[jaeger:map[protocols:map[grpc:map[endpoint:0.0.0.0:14250] thrift_binary:map[endpoint:0.0.0.0:6832] thrift_compact:map[endpoint:0.0.0.0:6831] thrift_http:map[endpoint:0.0.0.0:14268]]] zipkin:map[]]]]"
2022-04-05T15:32:54.656+0100 [DEBUG] Validate chart: ref=tempo
2022-04-05T15:32:54.656+0100 [DEBUG] Run chart: ref=tempo
2022-04-05T15:32:54.922+0100 [ERROR] 2022-04-05T15:32:54.922+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 1 resource(s)"
2022-04-05T15:32:54.934+0100 [ERROR] 2022-04-05T15:32:54.934+0100 [DEBUG] Helm debug: name=tempo chart=grafana/tempo message="creating 5 resource(s)"
2022-04-05T15:32:54.967+0100 [ERROR] 2022-04-05T15:32:54.967+0100 [INFO]  Creating Helm chart: ref=grafana
2022-04-05T15:32:54.967+0100 [DEBUG] Updating Helm chart repository: name=grafana url=https://grafana.github.io/helm-charts
2022-04-05T15:32:54.967+0100 [DEBUG] Using Kubernetes config: ref=grafana path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:32:54.968+0100 [ERROR] 2022-04-05T15:32:54.968+0100 [DEBUG] Creating chart from config: ref=grafana chart=grafana/grafana
2022-04-05T15:32:55.304+0100 [ERROR] 2022-04-05T15:32:55.304+0100 [DEBUG] Loading chart: ref=grafana path=/home/nicj/.shipyard/helm_charts/cache/grafana-6.21.2.tgz
2022-04-05T15:32:55.306+0100 [ERROR] 2022-04-05T15:32:55.306+0100 [DEBUG] Using Values: ref=grafana values="map[admin:map[existingSecret:grafana-password] datasources:map[datasources.yaml:map[apiVersion:1 datasources:[map[isDefault:true name:Prometheus type:prometheus url:http://prometheus-kube-prometheus-prometheus:9090] map[isDefault:false jsonData:map[derivedFields:[map[datasourceUid:tempo_uid matcherRegex:trace_id=(\\w+) name:trace_id url:$${__value.raw}]] maxLines:1000] name:Loki type:loki uid:loki_uid url:http://loki:3100] map[isDefault:false name:Tempo type:tempo uid:tempo_uid url:http://tempo:3100]]]] sidecar:map[dashboards:map[enabled:true]]]"
2022-04-05T15:32:55.306+0100 [DEBUG] Validate chart: ref=grafana
2022-04-05T15:32:55.306+0100 [DEBUG] Run chart: ref=grafana
2022-04-05T15:32:55.675+0100 [ERROR] W0405 15:32:55.675792   25671 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-05T15:32:55.678+0100 [ERROR] W0405 15:32:55.678489   25671 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-05T15:32:55.713+0100 [ERROR] 2022-04-05T15:32:55.713+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 1 resource(s)"
2022-04-05T15:32:55.734+0100 [ERROR] 2022-04-05T15:32:55.734+0100 [DEBUG] Helm debug: name=grafana chart=grafana/grafana message="creating 15 resource(s)"
2022-04-05T15:32:55.740+0100 [ERROR] W0405 15:32:55.740055   25671 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-05T15:32:55.741+0100 [ERROR] W0405 15:32:55.740951   25671 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
2022-04-05T15:32:55.820+0100 [ERROR] 2022-04-05T15:32:55.820+0100 [INFO]  Generating template: ref=monitor_ingress_gateway output=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-05T15:32:55.820+0100 [DEBUG] Template content: ref=monitor_ingress_gateway
  source=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: #{{ .Vars.monitoring_namespace }}
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: #{{ .Vars.consul_namespace }}
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-05T15:32:55.820+0100 [ERROR] 2022-04-05T15:32:55.820+0100 [DEBUG] Template output: ref=monitor_ingress_gateway
  destination=
  | # ServiceMonitor to configure Prometheus to scrape metrics from applications in the consul namespace
  | ---
  | apiVersion: monitoring.coreos.com/v1
  | kind: ServiceMonitor
  | metadata:
  |   labels:
  |     release: prometheus
  |   name: ingress-gateway
  |   namespace: monitoring
  | spec:
  |   endpoints:
  |   - interval: 15s
  |     port: metrics
  |   jobLabel: ingress-gateway
  |   namespaceSelector:
  |     matchNames:
  |     - consul
  |   selector:
  |     matchLabels:
  |       app: metrics
  |   
  | # Service to configure Prometheus to scrape metrics from the ingress-gateway in the consul namespace
  | ---
  | apiVersion: v1
  | kind: Service
  | metadata:
  |   name: ingress-gateway-metrics
  |   namespace: consul
  |   labels:
  |     app: metrics
  | spec:
  |   selector:
  |     component: ingress-gateway
  |   ports:
  |     - name: metrics
  |       protocol: TCP
  |       port: 20200
  |       targetPort: 20200
2022-04-05T15:32:55.820+0100 [ERROR] 2022-04-05T15:32:55.820+0100 [INFO]  Applying Kubernetes configuration: ref=monitor_ingress_gateway config=["/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml"]
2022-04-05T15:32:55.821+0100 [ERROR] 2022-04-05T15:32:55.821+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/.shipyard/data/consul_kubernetes/ingress-service-monitor.yaml
2022-04-05T15:32:55.941+0100 [ERROR] 2022-04-05T15:32:55.941+0100 [INFO]  Creating Helm chart: ref=consul-release-controller
2022-04-05T15:32:55.941+0100 [INFO]  Applying Kubernetes configuration: ref=application config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/../../example/kubernetes/"]
2022-04-05T15:32:55.941+0100 [DEBUG] Using Kubernetes config: ref=consul-release-controller path=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml
2022-04-05T15:32:55.941+0100 [ERROR] 2022-04-05T15:32:55.941+0100 [INFO]  Applying Kubernetes configuration: ref=upstreams-proxy config=["/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml"]
2022-04-05T15:32:55.941+0100 [ERROR] 2022-04-05T15:32:55.941+0100 [DEBUG] Creating chart from config: ref=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-05T15:32:55.941+0100 [DEBUG] Loading chart: ref=consul-release-controller path=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller
2022-04-05T15:32:55.942+0100 [ERROR] 2022-04-05T15:32:55.942+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/shipyard/kubernetes/fake-controller.yaml
2022-04-05T15:32:55.942+0100 [ERROR] 2022-04-05T15:32:55.941+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/api.yaml
2022-04-05T15:32:55.942+0100 [ERROR] 2022-04-05T15:32:55.942+0100 [DEBUG] Using Values: ref=consul-release-controller values="map[acls:map[enabled:true] autoencrypt:map[enabled:true] controller:map[container_config:map[image:map[repository:nicholasjackson/consul-release-controller tag:]] enabled:false] webhook:map[namespace:shipyard service:controller-webhook]]"
2022-04-05T15:32:55.942+0100 [DEBUG] Validate chart: ref=consul-release-controller
2022-04-05T15:32:55.942+0100 [DEBUG] Run chart: ref=consul-release-controller
2022-04-05T15:32:56.086+0100 [ERROR] 2022-04-05T15:32:56.086+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/application-dashboard.yaml
2022-04-05T15:32:56.107+0100 [ERROR] 2022-04-05T15:32:56.106+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/consul-config.yaml
2022-04-05T15:32:56.184+0100 [ERROR] 2022-04-05T15:32:56.184+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest-dashboard.yaml
2022-04-05T15:32:56.218+0100 [ERROR] 2022-04-05T15:32:56.218+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/loadtest.yaml
2022-04-05T15:32:56.265+0100 [ERROR] 2022-04-05T15:32:56.265+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/metrics.yaml
2022-04-05T15:32:56.273+0100 [ERROR] 2022-04-05T15:32:56.273+0100 [DEBUG] Applying Kubernetes config: config=/home/nicj/.shipyard/config/dc1/kubeconfig.yaml file=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/example/kubernetes/web.yaml
2022-04-05T15:32:56.404+0100 [ERROR] 2022-04-05T15:32:56.404+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 1 resource(s)"
2022-04-05T15:32:56.418+0100 [ERROR] 2022-04-05T15:32:56.418+0100 [DEBUG] Helm debug: name=consul-release-controller chart=/home/nicj/go/src/github.com/nicholasjackson/consul-release-controller/deploy/kubernetes/charts/consul-release-controller message="creating 13 resource(s)"
2022-04-05T15:32:56.582+0100 [ERROR] 2022-04-05T15:32:56.582+0100 [INFO]  Remote executing command: ref=exec_standalone command=sh args=["/output/fetch_certs.sh"] image="&{shipyardrun/tools:v0.6.0  }"
2022-04-05T15:32:56.618+0100 [ERROR] 2022-04-05T15:32:56.617+0100 [DEBUG] Image exists in local cache: image=shipyardrun/tools:v0.6.0
2022-04-05T15:32:56.617+0100 [DEBUG] Creating Docker Container: ref=exec_standalone.remote_exec
2022-04-05T15:32:56.791+0100 [ERROR] 2022-04-05T15:32:56.791+0100 [DEBUG] Remove container from default networks: ref=exec_standalone.remote_exec
2022-04-05T15:32:56.795+0100 [ERROR] 2022-04-05T15:32:56.795+0100 [DEBUG] Attaching container to network: ref=219f995de5201af86321523f1ef77066f082118c37b7d0a0eeecac4749d6e6a0 network=dc1
2022-04-05T15:32:56.805+0100 [ERROR] 2022-04-05T15:32:56.805+0100 [DEBUG] Disconnectng network: name=bridge ref=exec_standalone.remote_exec
2022-04-05T15:32:58.073+0100 [ERROR] 2022-04-05T15:32:58.073+0100 [DEBUG] Forcefully remove: container=219f995de5201af86321523f1ef77066f082118c37b7d0a0eeecac4749d6e6a0
2022-04-05T15:32:58.772+0100 [ERROR] 2022-04-05T15:32:58.772+0100 [DEBUG] Health check urls for browser windows: count=0
2022-04-05T15:32:58.772+0100 [DEBUG] Browser windows open

########################################################

Title Development setup
Author Nic Jackson
2022-04-05T15:32:58.772+0100 [ERROR] 
 Consul: https://localhost:8501
 Grafana: https://localhost:8080
 Application: http://localhost:18080

This blueprint defines 13 output variables.

You can set output variables as environment variables for your current terminal session using the following command:

eval $(shipyard env)

To list output variables use the command:

shipyard output
2022-04-05T15:32:59.384+0100 [INFO]  Starting controller
2022-04-05T15:33:03.960+0100 [DEBUG] statemachine: Handle event: event=event_configure state=state_start
2022-04-05T15:33:03.960+0100 [DEBUG] statemachine: Log state: event=event_configure state=state_start
2022-04-05T15:33:03.960+0100 [DEBUG] statemachine: Configure: state=state_configure
2022-04-05T15:33:03.960+0100 [DEBUG] statemachine: Log state: event=event_configure release=api state=state_configure
2022-04-05T15:33:03.960+0100 [INFO]  releaser-plugin-consul: Initializing deployment: service=api
2022-04-05T15:33:03.960+0100 [DEBUG] releaser-plugin-consul: Create service defaults: service=api
2022-04-05T15:33:06.989+0100 [DEBUG] releaser-plugin-consul: Create service resolver: service=api
2022-04-05T15:33:08.012+0100 [DEBUG] releaser-plugin-consul: Create service router: service=api
2022-04-05T15:33:08.971+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:33:09.024+0100 [DEBUG] releaser-plugin-consul: Create upstream service router: service=api
2022-04-05T15:33:10.052+0100 [DEBUG] releaser-plugin-consul: Create service intentions for the upstreams: service=api
2022-04-05T15:33:13.973+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:33:15.067+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-05T15:33:15.071+0100 [DEBUG] runtime-plugin-kubernetes: No candidate deployment, nothing to do
2022-04-05T15:33:15.071+0100 [DEBUG] statemachine: Configure completed successfully
2022-04-05T15:33:15.071+0100 [DEBUG] statemachine: Handle event: event=event_configured state=state_configure
2022-04-05T15:33:15.071+0100 [DEBUG] statemachine: Log state: event=event_configured state=state_configure
2022-04-05T15:33:15.071+0100 [DEBUG] statemachine: Log state: event=event_configured release=api state=state_idle
2022-04-05T15:33:18.974+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:33:18.995+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-05T15:33:18.995+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_idle
2022-04-05T15:33:18.995+0100 [DEBUG] statemachine: Handle event: event=event_deploy state=state_idle
2022-04-05T15:33:18.995+0100 [DEBUG] statemachine: Log state: event=event_deploy state=state_idle
2022-04-05T15:33:18.995+0100 [DEBUG] statemachine: Deploy: state=state_deploy
2022-04-05T15:33:18.995+0100 [DEBUG] statemachine: Log state: event=event_deploy release=api state=state_deploy
2022-04-05T15:33:18.997+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:19.000+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-05T15:33:19.000+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:20.001+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:20.004+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:20.004+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:21.004+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:21.008+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:21.008+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:22.009+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:22.012+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:22.012+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:23.012+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:23.015+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:23.015+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:23.996+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-05T15:33:24.001+0100 [DEBUG] runtime-plugin-kubernetes: Cloning deployment: name=api-deployment namespace=default
2022-04-05T15:33:24.006+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment-primary namespaces=default
2022-04-05T15:33:24.008+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:24.011+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=0
2022-04-05T15:33:24.011+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:24.016+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:24.020+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:24.020+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:25.011+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:25.014+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:25.014+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:25.021+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:25.024+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:25.024+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:26.015+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:26.018+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:26.018+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:26.025+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:26.027+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:26.027+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:27.018+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:27.021+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:27.021+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:27.028+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:27.030+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:27.030+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:28.021+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:28.025+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:28.025+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:28.031+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:28.033+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:28.033+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:29.026+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:29.029+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:29.029+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:29.034+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:29.036+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:29.036+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:30.030+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:30.033+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:30.033+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:30.037+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:30.040+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:30.040+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:31.033+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:31.037+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:31.037+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:31.041+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:31.044+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:31.044+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:32.037+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:32.041+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:32.041+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:32.044+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:32.048+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:32.048+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:33.041+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:33.044+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:33.044+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:33.048+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:33.051+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:33.051+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:34.044+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:34.048+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:34.048+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:34.052+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:34.054+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:34.054+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:35.048+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:35.051+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:35.051+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:35.055+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:35.058+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:35.058+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:36.051+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:36.054+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:36.054+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:36.059+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:36.062+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:36.062+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:37.055+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:37.058+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:37.058+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:37.062+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:37.064+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:37.064+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:38.059+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:38.062+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:38.062+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:38.065+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:38.067+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:38.067+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:33:39.063+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:39.066+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:39.066+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:39.068+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:33:39.071+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-05T15:33:39.071+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-05T15:33:39.081+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:39.084+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:39.084+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:40.067+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:40.070+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:40.070+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:40.085+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:40.088+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:40.088+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:41.070+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:41.073+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:41.073+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:41.088+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:41.091+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:41.091+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:42.073+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:42.076+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:42.076+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:42.092+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:42.095+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:42.095+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:43.076+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:43.079+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:43.079+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:43.096+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:43.099+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:33:43.099+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:44.080+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:44.083+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-05T15:33:44.083+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:44.083+0100 [DEBUG] runtime-plugin-kubernetes: Successfully cloned kubernetes deployment: name=api-deployment-primary namespace=default
2022-04-05T15:33:44.083+0100 [INFO]  runtime-plugin-kubernetes: Init primary complete: name=api-deployment namespace=default
2022-04-05T15:33:44.083+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-05T15:33:44.086+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-05T15:33:44.092+0100 [DEBUG] statemachine: Deploy completed, created primary, waiting for next candidate deployment
2022-04-05T15:33:44.099+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:33:44.102+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-05T15:33:44.102+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-05T15:33:49.093+0100 [INFO]  runtime-plugin-kubernetes: Remove candidate deployment: name=api-deployment namespace=default
2022-04-05T15:33:49.102+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-05T15:33:49.102+0100 [DEBUG] kubernetes-webhook: Ignore deployment, resource was modified by the controller: name=api-deployment namespace=default labels="map[app:api_v2 consul-release-controller-version:2296]"
2022-04-05T15:33:49.106+0100 [DEBUG] statemachine: Handle event: event=event_complete state=state_deploy
2022-04-05T15:33:49.106+0100 [DEBUG] statemachine: Log state: event=event_complete state=state_deploy
2022-04-05T15:33:49.106+0100 [DEBUG] statemachine: Log state: event=event_complete release=api state=state_idle
2022-04-05T15:34:00.153+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:00.169+0100 [DEBUG] kubernetes-webhook: Handle deployment admission: deployment=api-deployment namespaces=default
2022-04-05T15:34:00.169+0100 [DEBUG] kubernetes-webhook: Found existing release: name=api-deployment namespace=default state=state_idle
2022-04-05T15:34:00.169+0100 [DEBUG] statemachine: Handle event: event=event_deploy state=state_idle
2022-04-05T15:34:00.169+0100 [DEBUG] statemachine: Log state: event=event_deploy state=state_idle
2022-04-05T15:34:00.169+0100 [DEBUG] statemachine: Deploy: state=state_deploy
2022-04-05T15:34:00.169+0100 [DEBUG] statemachine: Log state: event=event_deploy release=api state=state_deploy
2022-04-05T15:34:00.172+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:00.175+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=0
2022-04-05T15:34:00.175+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:01.175+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:01.179+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:01.179+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:02.179+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:02.182+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:02.182+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:03.183+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:03.185+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:03.186+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:04.186+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:04.188+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:04.188+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:05.170+0100 [INFO]  runtime-plugin-kubernetes: Init the Primary deployment: name=api-deployment namespace=default
2022-04-05T15:34:05.172+0100 [DEBUG] runtime-plugin-kubernetes: Primary deployment already exists: name=api-deployment-primary namespace=default
2022-04-05T15:34:05.172+0100 [DEBUG] releaser-plugin-consul: Checking service is healthy: name=api
2022-04-05T15:34:05.175+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=100 traffic_canary=0
2022-04-05T15:34:05.185+0100 [DEBUG] statemachine: Deploy completed, executing strategy
2022-04-05T15:34:05.185+0100 [DEBUG] statemachine: Handle event: event=event_deployed state=state_deploy
2022-04-05T15:34:05.185+0100 [DEBUG] statemachine: Log state: event=event_deployed state=state_deploy
2022-04-05T15:34:05.185+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-05T15:34:05.185+0100 [DEBUG] statemachine: Log state: event=event_deployed release=api state=state_monitor
2022-04-05T15:34:05.185+0100 [DEBUG] statemachine: Executing post deployment tests
2022-04-05T15:34:05.185+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:34:05.189+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:05.191+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=503
2022-04-05T15:34:05.191+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:34:05.192+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:05.192+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:05.195+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=[] value_type=model.Vector warnings=[]
2022-04-05T15:34:06.193+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:06.196+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:06.196+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:07.196+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:07.200+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:07.200+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:08.201+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:08.204+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:08.204+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:09.204+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:09.207+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:09.207+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:10.207+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:10.210+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:10.210+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:11.211+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:11.213+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:11.213+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:12.214+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:12.217+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:12.217+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:13.217+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:13.220+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:13.220+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:14.220+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:14.223+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:14.223+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:15.196+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:34:15.200+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=503
2022-04-05T15:34:15.200+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:34:15.202+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=[] value_type=model.Vector warnings=[]
2022-04-05T15:34:15.224+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:15.226+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=0 desired_replicas=3
2022-04-05T15:34:15.226+0100 [DEBUG] kubernetes-client: Deployment not healthy: name=api-deployment namespace=default
2022-04-05T15:34:16.226+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:16.229+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-05T15:34:16.229+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-05T15:34:16.239+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment-primary namespace=default
2022-04-05T15:34:16.241+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment-primary namespace=default status_replicas=3 desired_replicas=3
2022-04-05T15:34:16.242+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment-primary namespace=default
2022-04-05T15:34:16.249+0100 [DEBUG] kubernetes-client: Checking health: name=api-deployment namespace=default
2022-04-05T15:34:16.251+0100 [DEBUG] kubernetes-client: Deployment health: name=api-deployment namespace=default status_replicas=3 desired_replicas=3
2022-04-05T15:34:16.251+0100 [DEBUG] kubernetes-client: Deployment healthy: name=api-deployment namespace=default
2022-04-05T15:34:16.261+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:21.262+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:25.202+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:34:25.227+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:34:25.227+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:34:25.229+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=[] value_type=model.Vector warnings=[]
2022-04-05T15:34:26.263+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:31.264+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:35.229+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:34:35.246+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:34:35.246+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:34:35.259+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=[] value_type=model.Vector warnings=[]
2022-04-05T15:34:36.265+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:41.266+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:45.260+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:34:45.283+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:34:45.283+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:34:45.285+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169285.284]"] value_type=model.Vector warnings=[]
2022-04-05T15:34:45.286+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:34:45.288+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169285.286]"] value_type=model.Vector warnings=[]
2022-04-05T15:34:46.268+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:51.268+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:34:55.288+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:34:55.304+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:34:55.304+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:34:55.307+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169295.305]"] value_type=model.Vector warnings=[]
2022-04-05T15:34:55.307+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:34:55.309+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169295.307]"] value_type=model.Vector warnings=[]
2022-04-05T15:34:56.269+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:01.270+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:05.309+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:35:05.329+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:35:05.329+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:35:05.331+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169305.33]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:05.331+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:35:05.333+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169305.332]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:05.333+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=-1
2022-04-05T15:35:05.333+0100 [DEBUG] strategy-plugin-canary: Waiting for initial grace before starting rollout: type=canary delay=30
2022-04-05T15:35:06.271+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:11.273+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:16.274+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:21.275+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:26.276+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:31.277+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:35.334+0100 [DEBUG] strategy-plugin-canary: Strategy setup: type=canary traffic=10
2022-04-05T15:35:35.334+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-05T15:35:35.334+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-05T15:35:35.334+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-05T15:35:35.334+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-05T15:35:35.334+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-05T15:35:35.334+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=90 traffic_canary=10
2022-04-05T15:35:35.346+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-05T15:35:35.346+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-05T15:35:35.346+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-05T15:35:35.346+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-05T15:35:35.346+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-05T15:35:35.346+0100 [DEBUG] statemachine: Executing post deployment tests
2022-04-05T15:35:35.346+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:35:35.363+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:35:35.363+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:35:35.365+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => NaN @[1649169335.364]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:35.365+0100 [DEBUG] monitor-plugin-prometheus: query value less than min: name=request-success preset=envoy-request-success value=-9223372036854775808
2022-04-05T15:35:36.278+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:41.278+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:45.365+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:35:45.383+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:35:45.383+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:35:45.386+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169345.384]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:45.386+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:35:45.388+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169345.386]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:46.280+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:51.280+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:35:55.389+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:35:55.404+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:35:55.405+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:35:55.407+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169355.405]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:55.407+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:35:55.409+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.849999999999998 @[1649169355.407]"] value_type=model.Vector warnings=[]
2022-04-05T15:35:56.282+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:01.283+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:05.409+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:36:05.426+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:36:05.426+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:36:05.428+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169365.426]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:05.428+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:36:05.430+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169365.428]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:05.430+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=10
2022-04-05T15:36:06.284+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:11.285+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:16.285+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:21.287+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:26.288+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:31.289+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:35.431+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-05T15:36:35.431+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:36:35.434+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169395.431]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:35.434+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:36:35.436+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169395.434]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:35.436+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=30
2022-04-05T15:36:35.436+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-05T15:36:35.436+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-05T15:36:35.436+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-05T15:36:35.436+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-05T15:36:35.436+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-05T15:36:35.436+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=70 traffic_canary=30
2022-04-05T15:36:35.442+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-05T15:36:35.442+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-05T15:36:35.442+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-05T15:36:35.442+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-05T15:36:35.442+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-05T15:36:35.442+0100 [DEBUG] statemachine: Executing post deployment tests
2022-04-05T15:36:35.442+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:36:35.458+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:36:35.458+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:36:35.460+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169395.458]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:35.460+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:36:35.462+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169395.461]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:36.290+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:41.291+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:45.463+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:36:45.480+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:36:45.480+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:36:45.482+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169405.481]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:45.482+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:36:45.484+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169405.483]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:46.292+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:51.294+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:36:55.484+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:36:55.502+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:36:55.502+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:36:55.504+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169415.503]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:55.504+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:36:55.506+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169415.505]"] value_type=model.Vector warnings=[]
2022-04-05T15:36:55.506+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=30
2022-04-05T15:36:56.295+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:01.296+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:06.296+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:11.298+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:16.298+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:21.299+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:25.508+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-05T15:37:25.508+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:37:25.510+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169445.508]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:25.510+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:37:25.512+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169445.511]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:25.512+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=50
2022-04-05T15:37:25.512+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-05T15:37:25.512+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-05T15:37:25.512+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-05T15:37:25.512+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-05T15:37:25.512+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-05T15:37:25.512+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=50 traffic_canary=50
2022-04-05T15:37:25.522+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-05T15:37:25.522+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-05T15:37:25.522+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-05T15:37:25.522+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-05T15:37:25.522+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-05T15:37:25.522+0100 [DEBUG] statemachine: Executing post deployment tests
2022-04-05T15:37:25.522+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:37:25.539+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:37:25.539+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:37:25.541+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169445.54]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:25.541+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:37:25.544+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169445.542]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:26.299+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:31.300+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:35.544+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:37:35.560+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:37:35.561+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:37:35.563+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169455.561]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:35.563+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:37:35.564+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169455.563]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:36.301+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:41.302+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:45.565+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:37:45.581+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:37:45.581+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:37:45.583+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169465.582]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:45.583+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:37:45.585+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169465.584]"] value_type=model.Vector warnings=[]
2022-04-05T15:37:45.585+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=50
2022-04-05T15:37:46.303+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:51.303+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:37:56.304+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:01.306+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:06.306+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:11.307+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:15.586+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-05T15:38:15.586+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:38:15.588+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169495.587]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:15.588+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:38:15.590+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169495.589]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:15.590+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=70
2022-04-05T15:38:15.590+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-05T15:38:15.590+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-05T15:38:15.590+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-05T15:38:15.590+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-05T15:38:15.590+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-05T15:38:15.590+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=30 traffic_canary=70
2022-04-05T15:38:15.594+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-05T15:38:15.594+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-05T15:38:15.594+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-05T15:38:15.594+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-05T15:38:15.594+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-05T15:38:15.595+0100 [DEBUG] statemachine: Executing post deployment tests
2022-04-05T15:38:15.595+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:38:15.611+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:38:15.611+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:38:15.613+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169495.611]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:15.613+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:38:15.615+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169495.614]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:16.307+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:21.308+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:25.615+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:38:25.632+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:38:25.632+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:38:25.634+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169505.633]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:25.634+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:38:25.636+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169505.635]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:26.309+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:31.310+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:35.637+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:38:35.653+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:38:35.653+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:38:35.655+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169515.654]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:35.655+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:38:35.658+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169515.656]"] value_type=model.Vector warnings=[]
2022-04-05T15:38:35.658+0100 [INFO]  strategy-plugin-canary: Executing strategy: type=canary traffic=70
2022-04-05T15:38:36.311+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:41.312+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:46.313+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:51.314+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:38:56.315+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:39:01.316+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:39:05.658+0100 [DEBUG] strategy-plugin-canary: Checking metrics: type=canary
2022-04-05T15:39:05.658+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:39:05.660+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169545.659]"] value_type=model.Vector warnings=[]
2022-04-05T15:39:05.660+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:39:05.662+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169545.661]"] value_type=model.Vector warnings=[]
2022-04-05T15:39:05.662+0100 [DEBUG] strategy-plugin-canary: Strategy success: type=canary traffic=90
2022-04-05T15:39:05.662+0100 [DEBUG] statemachine: Monitor checks completed, candidate healthy
2022-04-05T15:39:05.662+0100 [DEBUG] statemachine: Handle event: event=event_healthy state=state_monitor
2022-04-05T15:39:05.662+0100 [DEBUG] statemachine: Log state: event=event_healthy state=state_monitor
2022-04-05T15:39:05.663+0100 [DEBUG] statemachine: Scale: state=state_scale
2022-04-05T15:39:05.663+0100 [DEBUG] statemachine: Log state: event=event_healthy release=api state=state_scale
2022-04-05T15:39:05.663+0100 [INFO]  releaser-plugin-consul: Scale deployment: name=api traffic_primary=10 traffic_canary=90
2022-04-05T15:39:05.668+0100 [DEBUG] statemachine: Scale completed successfully
2022-04-05T15:39:05.668+0100 [DEBUG] statemachine: Handle event: event=event_scaled state=state_scale
2022-04-05T15:39:05.668+0100 [DEBUG] statemachine: Log state: event=event_scaled state=state_scale
2022-04-05T15:39:05.668+0100 [DEBUG] statemachine: Monitor: state=state_monitor
2022-04-05T15:39:05.668+0100 [DEBUG] statemachine: Log state: event=event_scaled release=api state=state_monitor
2022-04-05T15:39:05.668+0100 [DEBUG] statemachine: Executing post deployment tests
2022-04-05T15:39:05.668+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:39:05.683+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:39:05.684+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:39:05.685+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169545.684]"] value_type=model.Vector warnings=[]
2022-04-05T15:39:05.685+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:39:05.687+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.85 @[1649169545.686]"] value_type=model.Vector warnings=[]
2022-04-05T15:39:06.317+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:39:11.318+0100 [INFO]  release_handler: Release GET handler called
2022-04-05T15:39:15.688+0100 [DEBUG] monitor-plugin-test-http: Executing request to upstream: url=http://localhost:28080/ upstream=api.default
2022-04-05T15:39:15.705+0100 [DEBUG] monitor-plugin-test-http: Response from upstream: url=http://localhost:28080/ upstream=api.default status_code=200
2022-04-05T15:39:15.705+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-success
  query=
  | 
  | sum(
  | 	rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
  |       envoy_cluster_name="local_app",
  |       envoy_response_code!~"5.*"
  |     }[30s]
  |   )
  | )
  | /
  | sum(
  |   rate(
  |     envoy_cluster_upstream_rq{
  |       namespace="default",
  |       envoy_cluster_name="local_app",
  |       pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |     }[30s]
  |   )
  | )
  | * 100
  
2022-04-05T15:39:15.707+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-success preset=envoy-request-success value=["{} => 100 @[1649169555.705]"] value_type=model.Vector warnings=[]
2022-04-05T15:39:15.707+0100 [DEBUG] monitor-plugin-prometheus: querying prometheus: address=http://localhost:9090 name=request-duration
  query=
  | 
  | histogram_quantile(
  |   0.99,
  |   sum(
  |     rate(
  |       envoy_cluster_upstream_rq_time_bucket{
  |         namespace="default",
  |         envoy_cluster_name="local_app",
  |         pod=~"api-deployment-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
  |       }[30s]
  |     )
  |   ) by (le)
  | )
  
2022-04-05T15:39:15.709+0100 [DEBUG] monitor-plugin-prometheus: query value returned: name=request-duration preset=envoy-request-duration value=["{} => 24.849999999999998 @[1649169555.707]"] value_type=model.Vector warnings=[]
2022-04-05T15:39:16.318+0100 [INFO]  Shutting down server gracefully
2022-04-05T15:39:16.319+0100 [INFO]  Shutting down listener
2022-04-05T15:39:16.319+0100 [INFO]  Shutting down metrics
2022-04-05T15:39:16.320+0100 [INFO]  Shutting down kubernetes controller
2022-04-05T15:39:16.320+0100 [INFO]  kubernetes-controller: Stopping Kubernetes controller
